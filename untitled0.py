# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sRR-shdND_DfNOsvBqlTSI_idZ8wSJg5
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np

# Your path to the folder
LOAD_FOLDER = '/content/drive/MyDrive/Capstone_v2/processed_data/'

print("Loading all model-ready data...")
X_train = np.load(LOAD_FOLDER + 'X_train.npy')
y_train = np.load(LOAD_FOLDER + 'y_train.npy')
X_test = np.load(LOAD_FOLDER + 'X_test.npy')
y_test = np.load(LOAD_FOLDER + 'y_test.npy')
adj_matrix = np.load(LOAD_FOLDER + 'adj_matrix.npy')

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")
print(f"Adjacency Matrix shape: {adj_matrix.shape}")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from google.colab import drive
import os
import warnings

# --- 0. Setup and Configuration ---
warnings.filterwarnings('ignore')
print("TensorFlow Version:", tf.__version__)

# Set the path to your data
DATA_FOLDER = '/content/drive/MyDrive/Capstone_v2/processed_data/'
# This is where the new, trained model will be saved
MODEL_SAVE_PATH = os.path.join(DATA_FOLDER, 'model_A_gru_baseline.keras')

# --- 1. Mount Drive and Load Data ---
print("Mounting Google Drive...")
drive.mount('/content/drive')

print("Loading data from checkpoint...")
X_train = np.load(DATA_FOLDER + 'X_train.npy')
y_train = np.load(DATA_FOLDER + 'y_train.npy')
X_test = np.load(DATA_FOLDER + 'X_test.npy')
y_test = np.load(DATA_FOLDER + 'y_test.npy')

print(f"Original X_train shape: {X_train.shape}")
print(f"Original y_train shape: {y_train.shape}")

# --- 2. Reshape Data for a Simple GRU ---
# A simple GRU (Model A) is not "spatio-aware". It treats each
# node's time series as an independent sample.
# We must reshape the data from (Samples, Timesteps, Nodes, Features)
# to (Samples * Nodes, Timesteps, Features).

# Original X_train shape: (SAMPLES, 12, N_NODES, 32)
# Original y_train shape: (SAMPLES, N_NODES)

print("Reshaping data for non-spatial GRU...")
# 1. Transpose X to (SAMPLES, N_NODES, 12, 32)
X_train_gru = np.transpose(X_train, (0, 2, 1, 3))
X_test_gru = np.transpose(X_test, (0, 2, 1, 3))

# 2. Reshape X to (SAMPLES * N_NODES, 12, 32)
n_samples, n_nodes, n_timesteps, n_features = X_train_gru.shape
X_train_gru = X_train_gru.reshape((-1, n_timesteps, n_features))

n_samples_test = X_test_gru.shape[0]
X_test_gru = X_test_gru.reshape((-1, n_timesteps, n_features))

# 3. Reshape y: (SAMPLES, N_NODES) -> (SAMPLES * N_NODES, 1)
y_train_gru = y_train.reshape((-1, 1))
y_test_gru = y_test.reshape((-1, 1))

print(f"Reshaped X_train for GRU: {X_train_gru.shape}")
print(f"Reshaped y_train for GRU: {y_train_gru.shape}")

# --- 3. Build Model A (Baseline GRU) ---
print("Building GRU model...")

# Get input shape from the reshaped data
input_shape = (X_train_gru.shape[1], X_train_gru.shape[2]) # (12, 32)

inputs = Input(shape=input_shape)
# A single 64-unit GRU layer is a strong baseline
x = GRU(64, activation='tanh')(inputs)
outputs = Dense(1)(x) # Output is 1 value (the predicted congestion)

model_gru = Model(inputs, outputs)

# Compile the model
model_gru.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  loss='mean_squared_error',
                  metrics=['mean_absolute_error'])

model_gru.summary()

# --- 4. Define Callbacks (To save your progress!) ---
print("Defining callbacks...")

# This saves the *best* model file to your Drive
checkpoint = ModelCheckpoint(MODEL_SAVE_PATH,
                             monitor='val_mean_absolute_error',
                             save_best_only=True,
                             mode='min',
                             verbose=1)

# This stops training if the model isn't improving
early_stop = EarlyStopping(monitor='val_mean_absolute_error',
                           patience=10, # Wait 10 epochs for improvement
                           mode='min',
                           verbose=1,
                           restore_best_weights=True)

# --- 5. Train the Model (You can sleep now) ---
print("\n--- Starting Model Training (Model A: GRU Baseline) ---")
print("This cell will now train the model and save the best version.")
print(f"Best model will be saved to: {MODEL_SAVE_PATH}\n")

history = model_gru.fit(
    X_train_gru,
    y_train_gru,
    epochs=100,  # Set a high number, EarlyStopping will find the best
    batch_size=64, # Larger batch size for faster GPU training
    validation_data=(X_test_gru, y_test_gru),
    callbacks=[checkpoint, early_stop],
    verbose=1
)

print("\n--- Training Complete ---")
print(f"Best baseline model has been saved to: {MODEL_SAVE_PATH}")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense, Layer, TimeDistributed, Reshape, Permute, Lambda
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from google.colab import drive
import os
import warnings
import scipy.sparse as sp # For fast normalization

# --- 0. Setup and Configuration ---
warnings.filterwarnings('ignore')
print("TensorFlow Version:", tf.__version__)

# Set the path to your data
DATA_FOLDER = '/content/drive/MyDrive/Capstone_v2/processed_data/'
# --- FIX 1: Change filename to reflect weights-only save ---
MODEL_SAVE_PATH = os.path.join(DATA_FOLDER, 'model_B_gcn_gru_final.weights.h5')
# --- END OF FIX ---

# --- 1. SETUP TPU STRATEGY (CRITICAL) ---
print("Connecting to TPU...")
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()
    strategy = tf.distribute.TPUStrategy(tpu)
    print("TPU found and connected.")
    print('Running on TPU:', tpu.master())
    print('Number of replicas:', strategy.num_replicas_in_sync)
except ValueError:
    print("TPU not found. Switching to default strategy (CPU/GPU).")
    strategy = tf.distribute.get_strategy()

# --- 2. Mount Drive and Load Data (on CPU) ---
print("Mounting Google Drive...")
drive.mount('/content/drive')

print("Loading data from checkpoint...")
X_train = np.load(DATA_FOLDER + 'X_train.npy')
y_train = np.load(DATA_FOLDER + 'y_train.npy')
X_test = np.load(DATA_FOLDER + 'X_test.npy')
y_test = np.load(DATA_FOLDER + 'y_test.npy')
adj_matrix = np.load(DATA_FOLDER + 'adj_matrix.npy')

# --- FIX THE Y SHAPES ---
y_train = np.squeeze(y_train, axis=(1, 3))
y_test = np.squeeze(y_test, axis=(1, 3))
# --------------------------

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape (Corrected): {y_train.shape}")
print(f"y_test shape (Corrected): {y_test.shape}")
print(f"Adjacency Matrix shape: {adj_matrix.shape}")

N_NODES = adj_matrix.shape[0]

# --- 3. Pre-process Adjacency Matrix (FAST VERSION) ---
def normalize_adj_sparse(adj):
    """Symmetrically normalize adjacency matrix using sparse matrices."""
    print("Converting to sparse matrix...")
    adj = sp.csr_matrix(adj) # Convert dense numpy array to sparse
    adj = adj + sp.eye(adj.shape[0]) # Add self-loops
    print("Calculating degrees...")
    rowsum = np.array(adj.sum(1)) # Get degree of each node
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # Create sparse diagonal matrix
    print("Performing sparse matrix multiplication...")
    # D^(-1/2) * (A + I) * D^(-1/2)
    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32)

print("Normalizing adjacency matrix (fast sparse method)...")
adj_normalized = normalize_adj_sparse(adj_matrix)
adj_tensor = tf.convert_to_tensor(adj_normalized.toarray(), dtype=tf.float32)
print("Normalization complete.")


# --- 4. CREATE tf.data.Dataset ---
# --- Force Batch Size to 1 to prevent crashes ---
GLOBAL_BATCH_SIZE = 1
print(f"Global Batch Size: {GLOBAL_BATCH_SIZE}")

ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))
ds_train = ds_train.batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))
ds_test = ds_test.batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# --- 5. Build Model (INSIDE strategy.scope()) ---

with strategy.scope():

    # --- 5a. Define Custom GCN Layer ---
    class GCNLayer(Layer):
        def __init__(self, units, adj_matrix_tensor):
            super(GCNLayer, self).__init__()
            self.units = units
            self.adj_matrix = adj_matrix_tensor

        def build(self, input_shape):
            self.kernel = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer="glorot_uniform",
                trainable=True,
                name="gcn_kernel"
            )
            self.built = True

        def call(self, inputs):
            support = tf.matmul(inputs, self.kernel)
            output = tf.matmul(self.adj_matrix, support)
            return tf.nn.relu(output)

        def compute_output_shape(self, input_shape):
            return (input_shape[0], input_shape[1], self.units)

    # --- 5b. Build Model B (GCN-GRU) ---
    print("Building GCN-GRU model inside TPU scope...")

    input_shape = X_train.shape[1:]
    TIMESTEPS = input_shape[0]

    gcn_units = 32
    gru_units = 32

    inputs = Input(shape=input_shape)
    gcn_out = TimeDistributed(GCNLayer(gcn_units, adj_tensor))(inputs)
    x = Permute((2, 1, 3))(gcn_out) # (Batch, Nodes, Timesteps, Features)

    x = Lambda(lambda x: tf.reshape(x, (-1, TIMESTEPS, gcn_units)))(x)

    x = GRU(gru_units, activation='tanh')(x)
    x = Dense(1)(x)

    outputs = Lambda(lambda x: tf.reshape(x, (-1, N_NODES)))(x)

    model_gcn_gru = Model(inputs, outputs)

    # --- 5c. Compile the Model ---
    model_gcn_gru.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                          loss='mean_squared_error',
                          metrics=['mean_absolute_error'])

    model_gcn_gru.summary()

# --- 6. Define Callbacks ---
print("Defining callbacks...")

checkpoint = ModelCheckpoint(MODEL_SAVE_PATH,
                             monitor='val_mean_absolute_error',
                             # --- FIX 2: Add save_weights_only=True ---
                             save_weights_only=True,
                             # --- END OF FIX ---
                             save_best_only=True,
                             mode='min',
                             verbose=1)

early_stop = EarlyStopping(monitor='val_mean_absolute_error',
                           patience=10,
                           mode='min',
                           verbose=1,
                           restore_best_weights=True)

# --- 7. Train the Model (Using the tf.data.Dataset) ---
print("\n--- Starting Model Training (Model B: GCN-GRU Hero Model) ---")
print(f"Best model weights will be saved to: {MODEL_SAVE_PATH}\n")

history_gcn_gru = model_gcn_gru.fit(
    ds_train,
    epochs=100,
    validation_data=ds_test,
    callbacks=[checkpoint, early_stop],
    verbose=1
)

print("\n--- Training Complete ---")
print(f"Best GCN-GRU model weights have been saved to: {MODEL_SAVE_PATH}")

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, GRU, Dense, Layer, TimeDistributed, Reshape, Permute, Lambda
from sklearn.metrics import mean_absolute_error, mean_squared_error
from google.colab import drive
import os
import scipy.sparse as sp
import math
import warnings

warnings.filterwarnings('ignore')

# --- 0. Setup and Configuration ---
DATA_FOLDER = '/content/drive/MyDrive/Capstone_v2/processed_data/'

# --- !! IMPORTANT !! ---
# Verify these file names match what you have saved
MODEL_A_PATH = os.path.join(DATA_FOLDER, 'model_A_gru_baseline.keras')
MODEL_B_WEIGHTS_PATH = os.path.join(DATA_FOLDER, 'model_B_gcn_gru_final.weights.h5')
# ---

print("Mounting Google Drive...")
drive.mount('/content/drive')

# --- 1. Load Test Data and Graph ---
print("Loading test data and graph...")
X_test = np.load(DATA_FOLDER + 'X_test.npy')
y_test_orig = np.load(DATA_FOLDER + 'y_test.npy')
adj_matrix = np.load(DATA_FOLDER + 'adj_matrix.npy')

# Squeeze y_test to match model output shapes
y_test = np.squeeze(y_test_orig, axis=(1, 3))
y_test_flat = y_test.flatten() # A 1D array for easy metric calculation

N_NODES = adj_matrix.shape[0]
TIMESTEPS = X_test.shape[1]
FEATURES = X_test.shape[3]

# --- 2. Evaluate Model A (GRU Baseline) ---
print("\n" + "="*30)
print("--- Evaluating Model A (Baseline GRU) ---")
print("="*30)

try:
    # Load the full model (architecture + weights)
    model_A = load_model(MODEL_A_PATH)
    print("Model A loaded successfully.")

    # Reshape data for GRU
    print("Reshaping data for Model A...")
    X_test_gru = np.transpose(X_test, (0, 2, 1, 3))
    X_test_gru = X_test_gru.reshape((-1, TIMESTEPS, FEATURES))

    # Reshape y_test for GRU-style comparison
    y_test_gru_flat = np.transpose(y_test, (0, 1)).flatten()

    # Make predictions
    print("Making predictions with Model A...")
    y_pred_A_flat = model_A.predict(X_test_gru).flatten()

    # Calculate metrics
    mae_A = mean_absolute_error(y_test_gru_flat, y_pred_A_flat)
    rmse_A = math.sqrt(mean_squared_error(y_test_gru_flat, y_pred_A_flat))

    print(f"Model A MAE: {mae_A:.4f}")
    print(f"Model A RMSE: {rmse_A:.4f}")

except Exception as e:
    print(f"--- FAILED TO LOAD MODEL A ---")
    print(f"Error: {e}")
    print(f"Please check the path: {MODEL_A_PATH}")
    mae_A, rmse_A = 999, 999 # Set dummy values

# --- 3. Evaluate Model B (GCN-GRU Hero) ---
print("\n" + "="*30)
print("--- Evaluating Model B (GCN-GRU Hero) ---")
print("="*30)

# --- 3a. We must re-build the model to load the weights ---

# Re-create the normalized adjacency tensor
def normalize_adj_sparse(adj):
    adj = sp.csr_matrix(adj)
    adj = adj + sp.eye(adj.shape[0])
    rowsum = np.array(adj.sum(1))
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)
    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).astype(np.float32)

print("Normalizing adj matrix for model build...")
adj_normalized = normalize_adj_sparse(adj_matrix)
adj_tensor = tf.convert_to_tensor(adj_normalized.toarray(), dtype=tf.float32)

# Define the custom layer
class GCNLayer(Layer):
    def __init__(self, units, adj_matrix_tensor):
        super(GCNLayer, self).__init__()
        self.units = units
        self.adj_matrix = adj_matrix_tensor
    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), initializer="glorot_uniform", trainable=True)
    def call(self, inputs):
        support = tf.matmul(inputs, self.kernel)
        output = tf.matmul(self.adj_matrix, support)
        return tf.nn.relu(output)
    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], self.units)

# Define the build function
def build_gcn_gru_model():
    gcn_units = 32
    gru_units = 32
    input_shape_model = (TIMESTEPS, N_NODES, FEATURES)

    inputs = Input(shape=input_shape_model)
    gcn_out = TimeDistributed(GCNLayer(gcn_units, adj_tensor))(inputs)
    x = Permute((2, 1, 3))(gcn_out)
    x = Lambda(lambda x: tf.reshape(x, (-1, TIMESTEPS, gcn_units)))(x)
    x = GRU(gru_units, activation='tanh')(x)
    x = Dense(1)(x)
    outputs = Lambda(lambda x: tf.reshape(x, (-1, N_NODES)))(x)

    model = Model(inputs, outputs)
    return model

# Build the model
print("Re-building model architecture...")
strategy = tf.distribute.get_strategy()
with strategy.scope():
    model_gcn_gru = build_gcn_gru_model()

# Load the saved weights
print(f"Loading weights from {MODEL_B_WEIGHTS_PATH}...")
model_gcn_gru.load_weights(MODEL_B_WEIGHTS_PATH)
print("Model B weights loaded successfully.")

# --- 3b. Make Predictions ---
print("Making predictions with Model B...")
# Model B predicts in shape (Samples, Nodes)
y_pred_B = model_gcn_gru.predict(X_test)
y_pred_B_flat = y_pred_B.flatten()

# --- 3c. Calculate Metrics ---
# We compare with the original y_test_flat
mae_B = mean_absolute_error(y_test_flat, y_pred_B_flat)
rmse_B = math.sqrt(mean_squared_error(y_test_flat, y_pred_B_flat))

print(f"Model B MAE: {mae_B:.4f}")
print(f"Model B RMSE: {rmse_B:.4f}")

# --- 4. Final Comparison ---
print("\n" + "="*45)
print("      üèÜ FINAL MODEL COMPARISON (1-File Dataset)")
print("="*45)
print("| Model                 | MAE (Lower is Better) |")
print("|-----------------------|-----------------------|")
print(f"| Model A (GRU Baseline)  | {mae_A:<21.4f} |")
print(f"| Model B (GCN-GRU Hero)  | {mae_B:<21.4f} |")
print("="*45)

print("\nAnalysis:")
if mae_B < mae_A:
    print("üéâ SUCCESS! The GCN-GRU model (Model B) is more accurate.")
    print("This proves that adding the spatial graph data improved the predictions.")
else:
    print("‚ö†Ô∏è RESULT: The GCN-GRU model performed worse or the same.")
    print("This is an unexpected result, but it is still a valid finding for your project.")