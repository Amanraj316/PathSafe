# -*- coding: utf-8 -*-
"""Step1_Data_Engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13BTfXhHNpTvRwBt4LppHu6RqShYU5D61
"""

# Cell 1 & 2: Install Libraries, Mount Drive & Define Paths
!pip install geopandas shapely networkx tqdm -q

import geopandas as gpd
import pandas as pd
import numpy as np
import networkx as nx
from shapely.geometry import Point, LineString
import glob
import json
import os
import pickle
from tqdm.notebook import tqdm
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from google.colab import drive

print("--- Cell 1 Complete: All libraries installed and imported. ---")

drive.mount('/content/drive')

# --- !!! IMPORTANT: UPDATE THESE PATHS !!! ---
BASE_DRIVE_PATH = '/content/drive/MyDrive/Capstone_v2/'

# Input files
GRAPH_FILE = os.path.join(BASE_DRIVE_PATH, 'geojson/new_delhi.json')
PROBE_FILES_PATH = os.path.join(BASE_DRIVE_PATH, 'probe_counts/geojson/*.geojson')
# --- CORRECTED FILE: ---
WEEKDAY_FILE = os.path.join(BASE_DRIVE_PATH, 'weekday_stats/2024_week_day_congestion_city.csv')

# Output directory
SAVE_DIR = os.path.join(BASE_DRIVE_PATH, 'processed_data/')
os.makedirs(SAVE_DIR, exist_ok=True)
# ---

print(f"--- Cell 2 Complete: Paths defined. Save directory is: {SAVE_DIR} ---")

# Cell 3: Part 1 - Build Graph & Edge Data (v8 - EFFICIENT VERSION)
print("Starting Part 1 (v8): Building Graph and Parsing Data...")

# --- 1. Load ALL probe_counts files (ONLY ONCE) ---
probe_files = glob.glob(PROBE_FILES_PATH)[:1]
if not probe_files:
    print(f"--- ERROR --- \nNo files found at {PROBE_FILES_PATH}.")
    raise FileNotFoundError(f"No files found at {PROBE_FILES_PATH}")

all_probe_gdfs = [] # This will be a list of GeoDataFrames
print(f"Loading {len(probe_files)} probe_count files... (This is the slow step, please be patient)")
for f in tqdm(probe_files):
    import warnings
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", UserWarning)
        # We load each file into memory
        all_probe_gdfs.append(gpd.read_file(f))
print("All files loaded into memory.")

# --- 2. Build the Graph ---
# Combine all in-memory GeoDataFrames into one
gdf_all_roads_data = gpd.GeoDataFrame(pd.concat(all_probe_gdfs, ignore_index=True), crs=all_probe_gdfs[0].crs)
# Drop header rows
gdf_all_roads_data = gdf_all_roads_data[~gdf_all_roads_data['geometry'].isna()].reset_index(drop=True)
# Find the unique roads (for graph building)
gdf_unique_roads = gdf_all_roads_data.drop_duplicates(subset=['newSegmentId'])
print(f"Loaded and combined all road segments. Total unique roads: {gdf_unique_roads.shape[0]}")

# --- Build the Node & Adjacency Matrix (Same as before) ---
node_coords = set()
edge_to_nodes_map = {} # newSegmentId -> (start_node_coord_str, end_node_coord_str)

print("Processing road segments to find nodes (intersections)...")
for idx, row in tqdm(gdf_unique_roads.iterrows(), total=gdf_unique_roads.shape[0]):
    try:
        road_id = row['newSegmentId']
        coords = list(row.geometry.coords)
        start_coord = f"{coords[0][0]:.6f},{coords[0][1]:.6f}"
        end_coord = f"{coords[-1][0]:.6f},{coords[-1][1]:.6f}"
        node_coords.add(start_coord)
        node_coords.add(end_coord)
        edge_to_nodes_map[road_id] = (start_coord, end_coord)
    except Exception as e:
        print(f"Skipping row {idx} due to geometry error: {e}")

node_list = sorted(list(node_coords))
node_to_id_map = {coord_str: i for i, coord_str in enumerate(node_list)}
id_to_node_map = {i: coord_str for i, coord_str in enumerate(node_list)}
N_NODES = len(node_list)
print(f"Found {N_NODES} unique nodes (intersections).")

adj_matrix = np.zeros((N_NODES, N_NODES), dtype=np.int8)
node_to_edges_map = {i: [] for i in range(N_NODES)} # node_id -> [list_of_road_ids]

print("Building Adjacency Matrix...")
for road_id, (start_coord, end_coord) in tqdm(edge_to_nodes_map.items()):
    if start_coord in node_to_id_map and end_coord in node_to_id_map:
        n1_id = node_to_id_map[start_coord]
        n2_id = node_to_id_map[end_coord]
        adj_matrix[n1_id, n2_id] = 1
        adj_matrix[n2_id, n1_id] = 1
        node_to_edges_map[n1_id].append(road_id)
        node_to_edges_map[n2_id].append(road_id)

print(f"Adjacency Matrix built. Shape: {adj_matrix.shape}")

# --- 3. Parse Traffic Data (From Memory) ---
print("Parsing traffic data from in-memory files... (This should be much faster)")
all_traffic_records = []

# --- THIS IS THE EFFICIENT LOGIC ---
# We loop through the 'all_probe_gdfs' list we already loaded
for gdf_single_file in tqdm(all_probe_gdfs):

    # Get the header from *this specific file*
    try:
        header_row = gdf_single_file.iloc[0]
        date_ranges_str = header_row['dateRanges']
        time_sets_str = header_row['timeSets']
        date_ranges = json.loads(date_ranges_str) if date_ranges_str else []
        time_sets = json.loads(time_sets_str) if time_sets_str else []
    except Exception as e:
        print(f"Skipping one file, could not parse its header. Error: {e}")
        continue

    # Create lookups *for this file only*
    date_lookup = {dr['@id']: dr['name'].split(' to ')[0] for dr in date_ranges}
    time_lookup = {ts['@id']: ts['name'] for ts in time_sets}

    # Iterate over the *rows in this file*
    for idx, row in gdf_single_file.iterrows():
        if row['geometry'] is None: # Skip header row
            continue

        road_id = row['newSegmentId']
        probe_counts_json_string = row['segmentProbeCounts']

        if not probe_counts_json_string:
            continue

        try:
            probe_counts_list = json.loads(probe_counts_json_string)
        except json.JSONDecodeError:
            continue

        for record in probe_counts_list:
            date_id = record.get('dateRange')
            time_id = record.get('timeSet')
            probe_count = record.get('probeCount')

            # Use the local lookup
            date_str = date_lookup.get(date_id)
            time_str = time_lookup.get(time_id, "")

            if date_str and time_str and probe_count is not None:
                try:
                    hour = int(time_str.split(':')[0])
                    timestamp_dt = pd.to_datetime(f"{date_str} {hour:02d}:00:00")
                    day_of_week = timestamp_dt.strftime('%A')

                    all_traffic_records.append({
                        'road_id': road_id,
                        'timestamp': timestamp_dt,
                        'vehicle_count': probe_count,
                        'hour': hour,
                        'day_of_week': day_of_week
                    })
                except ValueError:
                    continue # Skip record with invalid time

# --- END OF EFFICIENT LOGIC ---

# Create our final DataFrame of *edge* data
df_edges = pd.DataFrame(all_traffic_records)
# De-duplicate records (if any roads were in multiple files with same data)
df_edges = df_edges.drop_duplicates()

print(f"Created final 'Edge' data. Shape: {df_edges.shape}")
print(df_edges.head())
# ... (all your existing code from Cell 3) ...
print(f"Created final 'Edge' data. Shape: {df_edges.shape}")
print(df_edges.head())

# --- 1. MOUNT YOUR GOOGLE DRIVE ---
from google.colab import drive
import os
import pickle

drive.mount('/content/drive')
print("Google Drive Mounted.")

# --- 2. DEFINE YOUR SAVE PATH (Your exact path) ---
SAVE_FOLDER = '/content/drive/MyDrive/Capstone_v2/'

# Create the folder if it doesn't exist
os.makedirs(SAVE_FOLDER, exist_ok=True)
print(f"Ensured save directory exists: {SAVE_FOLDER}")

# --- 3. SAVE ALL THE OUTPUTS ---
print("Saving checkpoint files to Google Drive...")

# Save the main data (Parquet is fast and small)
# This assumes your DataFrame is named 'df_edges'
df_edges.to_parquet(SAVE_FOLDER + 'df_edges_checkpoint.parquet')
print("Saved 'df_edges_checkpoint.parquet'")

# Save the graph
# This assumes your matrix is named 'adj_matrix'
np.save(SAVE_FOLDER + 'adj_matrix_checkpoint.npy', adj_matrix)
print("Saved 'adj_matrix_checkpoint.npy'")

# Save the 'node_to_edges_map'
with open(SAVE_FOLDER + 'node_to_edges_map.pkl', 'wb') as f:
    pickle.dump(node_to_edges_map, f)
print("Saved 'node_to_edges_map.pkl'")

# Save the 'node_to_id_map'
with open(SAVE_FOLDER + 'node_to_id_map.pkl', 'wb') as f:
    pickle.dump(node_to_id_map, f)
print("Saved 'node_to_id_map.pkl'")

print(f"--- ALL CHECKPOINTS SAVED to {SAVE_FOLDER} ---")
print("You never have to run this parsing cell again.")
print("\n--- Cell 3 Complete: Graph built and ALL Edge data parsed. ---")

# --- NEW CELL: Run this INSTEAD of your slow parsing cell ---

import pandas as pd
import numpy as np
import pickle
import os
from google.colab import drive

# --- 1. MOUNT YOUR GOOGLE DRIVE ---
drive.mount('/content/drive')

# --- 2. DEFINE YOUR LOAD PATH (Your exact path) ---
LOAD_FOLDER = '/content/drive/MyDrive/Capstone_v2/'

# --- 3. LOAD ALL THE FILES ---
print("Loading all data from checkpoint files...")

# Load the main data
df_edges = pd.read_parquet(LOAD_FOLDER + 'df_edges_checkpoint.parquet')
print(f"Loaded 'df_edges'. Shape: {df_edges.shape}")

# Load the graph
adj_matrix = np.load(LOAD_FOLDER + 'adj_matrix_checkpoint.npy')
print(f"Loaded 'adj_matrix'. Shape: {adj_matrix.shape}")

# --- ADD THIS LINE ---
N_NODES = adj_matrix.shape[0]
print(f"Set N_NODES = {N_NODES}")
# --- END OF NEW LINE ---

# Load the 'node_to_edges_map'
with open(LOAD_FOLDER + 'node_to_edges_map.pkl', 'rb') as f:
    node_to_edges_map = pickle.load(f)
print("Loaded 'node_to_edges_map'.")

# Load the 'node_to_id_map'
with open(LOAD_FOLDER + 'node_to_id_map.pkl', 'rb') as f:
    node_to_id_map = pickle.load(f)
print("Loaded 'node_to_id_map'.")

# --- 4. (CRITICAL) RE-CREATE THE VARIABLE FOR CELL 4 ---
# Your "Cell 4" needs the 'df_node_edge_map' variable.
# We re-create it here from the map we just loaded. This is very fast.

print("Re-creating 'df_node_edge_map'...")
node_edge_map_list = []
for node_id, edge_list in node_to_edges_map.items():
    for road_id in edge_list:
        node_edge_map_list.append({'node_id': node_id, 'road_id': road_id})

df_node_edge_map = pd.DataFrame(node_edge_map_list)
print(f"Created 'df_node_edge_map' with {len(df_node_edge_map)} links.")

print("\n--- CHECKPOINT LOAD COMPLETE. ---")
print("You can now safely run your 'Cell 4' (the merge cell).")

# Cell 4: Part 2 - Aggregate, Preprocess, & Save
print("Starting Part 2 (v4): Aggregating, Preprocessing, and Saving...")

# --- 1. Create a Node-to-Road mapping DataFrame ---
node_edge_pairs = []
for node_id, road_ids in node_to_edges_map.items():
    for road_id in road_ids:
        node_edge_pairs.append({'node_id': node_id, 'road_id': road_id})
df_node_edge_map = pd.DataFrame(node_edge_pairs)
print(f"Created node-to-edge map with {df_node_edge_map.shape[0]} links.")

# --- 2. Merge Node map with Edge data ---
df_node_data_raw = df_node_edge_map.merge(df_edges, on='road_id', how='left')
print("Merged node map with traffic data.")

# --- 3. Aggregate by Node and Timestamp ---
print("Aggregating data by node and timestamp...")
df_nodes_final = df_node_data_raw.groupby(['node_id', 'timestamp']).agg(
    vehicle_count=('vehicle_count', 'mean'), # This is our new node-level data
    hour=('hour', 'first'),
    day_of_week=('day_of_week', 'first'),
).reset_index()

df_nodes_final['vehicle_count'] = df_nodes_final['vehicle_count'].fillna(0)
print(f"Final 'Node' data created. Shape: {df_nodes_final.shape}")

# --- 4. Pivot and Scale ---
print("Pivoting and scaling data...")
df_pivot = df_nodes_final.pivot(index='timestamp', columns='node_id', values='vehicle_count')
df_pivot = df_pivot.fillna(0)
df_pivot = df_pivot.reindex(columns=range(N_NODES), fill_value=0)

# This is our main 'y' target
data_y = df_pivot.values
print(f"Pivoted target data 'y' shape: {data_y.shape}") # (Timestamps, Nodes)

# --- 5. Prepare Feature Matrix 'X' ---
# Our features are:
# 1. Past vehicle_count (Node-level feature)
# 2. Hour (Time-level feature)
# 3. Day of week (Time-level feature)

# Get the base data (non-pivoted) for time features
df_features_base = df_nodes_final.drop_duplicates(subset=['timestamp'])
df_features_base = df_features_base.set_index('timestamp')
df_features_base = df_features_base.reindex(df_pivot.index)

# --- Encode/Scale Time Features ---
all_scalers = {}

# One-Hot Encode 'hour'
hour_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
hour_data = hour_encoder.fit_transform(df_features_base[['hour']].fillna(-1))
all_scalers['hour'] = hour_encoder

# One-Hot Encode 'day_of_week'
day_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
day_data = day_encoder.fit_transform(df_features_base[['day_of_week']].fillna('Unknown'))
all_scalers['day_of_week'] = day_encoder

# Combine time features
data_X_time_features = np.concatenate([hour_data, day_data], axis=1)
print(f"Combined time features shape: {data_X_time_features.shape}")

# --- Scale Node Feature (vehicle_count) ---
scaler_y = MinMaxScaler()
data_X_node_features = scaler_y.fit_transform(data_y)
all_scalers['vehicle_count'] = scaler_y
print(f"Scaled node features shape: {data_X_node_features.shape}")

# --- 6. Build Final Tensors ---
N_TIMESTAMPS = data_X_node_features.shape[0]
N_TIME_FEATURES = data_X_time_features.shape[1]

# Expand time features to match all nodes
data_X_time_expanded = np.expand_dims(data_X_time_features, axis=1)
data_X_time_tiled = np.tile(data_X_time_expanded, (1, N_NODES, 1))

# Expand node features
data_X_node_expanded = np.expand_dims(data_X_node_features, axis=2)

# --- Concatenate all features together ---
# This is our final X tensor!
data_X = np.concatenate([data_X_node_expanded, data_X_time_tiled], axis=2)
# Our y target is just the scaled node vehicle counts
data_y = data_X_node_expanded

print(f"\nFinal 'data_X' tensor shape (Timestamps, Nodes, Features): {data_X.shape}")
print(f"Final 'data_y' tensor shape (Timestamps, Nodes, 1): {data_y.shape}")

# --- 7. Create Sequences & Save ---
print("Creating sequences and saving files...")

def create_sequences(X_data, y_data, input_len, output_len):
    X = []
    y = []
    total_timesteps = X_data.shape[0]

    for i in range(total_timesteps - input_len - output_len + 1):
        X.append(X_data[i : (i + input_len)])
        y.append(y_data[(i + input_len) : (i + input_len + output_len)])

    return np.array(X), np.array(y)

INPUT_SEQ_LEN = 12  # Use 12 past observations
OUTPUT_SEQ_LEN = 1 # Predict 1 hour into the future

X, y = create_sequences(data_X, data_y, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN)

print(f"Created sequences:")
print(f"X shape (Samples, Timesteps, Nodes, Features): {X.shape}")
print(f"y shape (Samples, Timesteps, Nodes, 1): {y.shape}")

# Split the data
X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.3, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=(2/3), shuffle=False)
print(f"\nTrain set: X={X_train.shape}, y={y_train.shape}")
print(f"Validation set: X={X_val.shape}, y={y_val.shape}")
print(f"Test set: X={X_test.shape}, y={y_test.shape}")

# --- Save all processed data ---
print(f"\nSaving all files to: {SAVE_DIR}")
np.save(os.path.join(SAVE_DIR, 'X_train.npy'), X_train)
np.save(os.path.join(SAVE_DIR, 'y_train.npy'), y_train)
np.save(os.path.join(SAVE_DIR, 'X_val.npy'), X_val)
np.save(os.path.join(SAVE_DIR, 'y_val.npy'), y_val)
np.save(os.path.join(SAVE_DIR, 'X_test.npy'), X_test)
np.save(os.path.join(SAVE_DIR, 'y_test.npy'), y_test)
np.save(os.path.join(SAVE_DIR, 'adj_matrix.npy'), adj_matrix)
np.save(os.path.join(SAVE_DIR, 'node_coordinates.npy'), np.array(node_list))

with open(os.path.join(SAVE_DIR, 'scalers.pkl'), 'wb') as f:
    pickle.dump(all_scalers, f)

print("\n--- ALL OF STEP 1 (v4) IS COMPLETE! ---")

# --- NEW CELL: Create the Length (Distance) Matrix (Corrected Paths) ---
import geopandas as gpd
import pandas as pd
import numpy as np
import glob
import json
import pickle
import os
from tqdm import tqdm
from google.colab import drive
from geopy.distance import geodesic # For calculating road length

print("Mounting Google Drive...")
drive.mount('/content/drive')

# --- 1. SET YOUR PATHS (Corrected) ---
# Path to your raw .geojson files
RAW_DATA_PATH = '/content/drive/MyDrive/Capstone_v2/probe_counts/geojson/'
# Path where your .pkl files are stored (the root folder)
CHECKPOINT_FOLDER = '/content/drive/MyDrive/Capstone_v2/'
# Path where the final 'processed_data' is saved
SAVE_FOLDER = '/content/drive/MyDrive/Capstone_v2/processed_data/'

# --- 2. Load the Node Map (We need this to build the matrix) ---
print("Loading node maps...")
try:
    with open(os.path.join(CHECKPOINT_FOLDER, 'node_to_id_map.pkl'), 'rb') as f:
        node_to_id_map = pickle.load(f)
    print(f"Loaded node_to_id_map with {len(node_to_id_map)} nodes.")
    N_NODES = len(node_to_id_map)
except FileNotFoundError:
    print("--- ERROR ---")
    print(f"Could not find 'node_to_id_map.pkl' in {CHECKPOINT_FOLDER}.")
    print("Please make sure the file exists.")
    # Stop the cell if this file is missing
    raise


# --- 3. Find all raw .geojson files ---
# We'll use just the FIRST file. The road network is the same in all of them.
probe_files = glob.glob(os.path.join(RAW_DATA_PATH, '*.geojson'))
if not probe_files:
    raise FileNotFoundError(f"No GeoJSON files found in {RAW_DATA_PATH}")

first_file = probe_files[0]
print(f"Processing road lengths from file: {first_file}")

# --- 4. Load and Process the File ---
# Use geopandas to load the file
gdf_roads = gpd.read_file(first_file)
# Filter for only roads (LINESTRINGs) and drop header rows
gdf_roads = gdf_roads[~gdf_roads['geometry'].isna()].reset_index(drop=True)

print(f"Loaded {len(gdf_roads)} road segments.")

# --- 5. Build the Length Matrix ---
# Create an empty matrix
# We use float32 to save space. 'inf' means no direct path.
length_matrix = np.full((N_NODES, N_NODES), np.inf, dtype=np.float32)
# Set distance from a node to itself to 0
np.fill_diagonal(length_matrix, 0)

print(f"Building {N_NODES}x{N_NODES} length matrix...")
for idx, row in tqdm(gdf_roads.iterrows(), total=gdf_roads.shape[0]):
    try:
        # Get start/end node coordinates
        coords = list(row.geometry.coords)
        start_coord_str = f"{coords[0][0]:.6f},{coords[0][1]:.6f}"
        end_coord_str = f"{coords[-1][0]:.6f},{coords[-1][1]:.6f}"

        # Find the node IDs from our loaded map
        n1_id = node_to_id_map.get(start_coord_str)
        n2_id = node_to_id_map.get(end_coord_str)

        if n1_id is not None and n2_id is not None:
            # Calculate total length of the road by summing segments
            total_length_meters = 0
            for i in range(len(coords) - 1):
                # geopy needs (lat, lon)
                pt1_latlon = (coords[i][1], coords[i][0])
                pt2_latlon = (coords[i+1][1], coords[i+1][0])
                total_length_meters += geodesic(pt1_latlon, pt2_latlon).meters

            # Store the length in the matrix (in both directions)
            # Only update if this is a shorter path (it shouldn't be, but safe)
            if total_length_meters < length_matrix[n1_id, n2_id]:
                length_matrix[n1_id, n2_id] = total_length_meters
                length_matrix[n2_id, n1_id] = total_length_meters

    except Exception as e:
        continue # Skip any rows with bad geometry

# --- 6. Save the new matrix ---
LENGTH_MATRIX_SAVE_PATH = os.path.join(SAVE_FOLDER, 'length_matrix.npy')
np.save(LENGTH_MATRIX_SAVE_PATH, length_matrix)

print("\n--- SUCCESS ---")
print(f"Length matrix saved to: {LENGTH_MATRIX_SAVE_PATH}")

import pickle
import os
from google.colab import drive

print("Mounting Google Drive...")
drive.mount('/content/drive')

# --- 1. SET YOUR PATHS ---
# This is where your 'node_to_id_map.pkl' is
CHECKPOINT_FOLDER = '/content/drive/MyDrive/Capstone_v2/'
# This is where your final app data is
SAVE_FOLDER = '/content/drive/MyDrive/Capstone_v2/processed_data/'

# --- 2. Load the existing file ---
NODE_TO_ID_PATH = os.path.join(CHECKPOINT_FOLDER, 'node_to_id_map.pkl')
print(f"Loading {NODE_TO_ID_PATH}...")

try:
    with open(NODE_TO_ID_PATH, 'rb') as f:
        node_to_id_map = pickle.load(f)
except FileNotFoundError:
    print(f"ERROR: Cannot find the file at {NODE_TO_ID_PATH}")
    print("Please make sure the path is correct.")
    raise

print(f"Loaded map with {len(node_to_id_map)} nodes.")

# --- 3. Invert the dictionary ---
print("Inverting dictionary...")
# This swaps the keys and values
id_to_node_map = {node_id: coord_str for coord_str, node_id in node_to_id_map.items()}
print("Inversion complete.")

# --- 4. Save the new file ---
ID_TO_NODE_PATH = os.path.join(SAVE_FOLDER, 'id_to_node_map.pkl')
print(f"Saving new file to {ID_TO_NODE_PATH}...")

with open(ID_TO_NODE_PATH, 'wb') as f:
    pickle.dump(id_to_node_map, f)

print("\n--- SUCCESS ---")
print("id_to_node_map.pkl has been created.")